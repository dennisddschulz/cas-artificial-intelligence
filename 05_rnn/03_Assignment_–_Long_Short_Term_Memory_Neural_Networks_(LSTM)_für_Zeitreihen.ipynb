{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgqTvcczSdXr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment – Long Short-Term Memory Neural Networks (LSTM) für Zeitreihen\n",
        "\n",
        "Bern University of Applied Sciences – CAS AI HS 25\n",
        "Dozent: Ilja Rasin  \n",
        "\n",
        "Dieses Notebook gehört zur dritten Vorlesung (RNN).\n",
        "\n",
        "**Ziele dieser Einheit**\n",
        "\n",
        "- Grundidee von RNN / LSTM im Kontext von Zeitreihen verstehen  \n",
        "- Eine synthetische Zeitreihe (Sinus + Rauschen) vorhersagen  \n",
        "- Ein einfaches LSTM-Modell in PyTorch implementieren  \n",
        "- Train/Validation/Test-Split und Fensterbildung für Time Series anwenden  \n",
        "- Optional: Eigene Zeitreihe laden (z.B. S&P500 mit `yfinance`) und ausprobieren\n",
        "\n",
        "**Hinweis für Studierende (Assignment-Charakter)**\n",
        "\n",
        "- Lest das Notebook von oben nach unten.  \n",
        "- Führt alle Zellen aus und versucht, jeden Schritt zu verstehen.  \n",
        "- Die Aufgabenabschnitte (*Aufgaben*) enthalten konkrete Experimente, die ihr selbst durchführen sollt.\n"
      ],
      "metadata": {
        "id": "Q7mMmVahSvlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports und Basiskonfiguration\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Matplotlib Standard-Einstellungen (keine speziellen Farben)\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
        "\n",
        "# Reproduzierbarkeit: Zufalls-Saat setzen\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "id": "v5VWRzIOS0n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Synthetische Zeitreihe: Sinus + Rauschen\n",
        "\n",
        "Wir beginnen mit einer **einfachen künstlichen Zeitreihe**, an der sich das Verhalten eines RNN gut demonstrieren lässt:\n",
        "\n",
        "$\n",
        "x_t = \\sin(\\omega t) + \\epsilon_t\n",
        "$\n",
        "\n",
        "mit etwas Rauschen ($\\epsilon_t\\$).\n",
        "\n",
        "Später können wir **eigene Zeitreihen** (z.B. S&P500) an derselben Pipeline ausprobieren.\n"
      ],
      "metadata": {
        "id": "7UddiNsGS-nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Zeitachse und Sinus-Zeitreihe erzeugen\n",
        "\n",
        "n_points = 1000          # Anzahl der Zeitpunkte\n",
        "t_max = 50               # \"Zeit\" (x-Achse) bis wohin wir gehen\n",
        "t = np.linspace(0, t_max, n_points)\n",
        "\n",
        "# Sauberer Sinus\n",
        "signal_clean = np.sin(2 * math.pi * t / 10.0)  # Periode ~10\n",
        "\n",
        "# Rauschen hinzufügen\n",
        "noise = 0.3 * np.random.randn(n_points)\n",
        "signal_noisy = signal_clean + noise\n",
        "\n",
        "# In Torch-Tensor umwandeln\n",
        "series = torch.tensor(signal_noisy, dtype=torch.float32)\n",
        "series.shape\n"
      ],
      "metadata": {
        "id": "M1t6c8KTS7wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Visualisierung der Zeitreihe\n",
        "\n",
        "plt.plot(t, series.numpy())\n",
        "plt.title(\"Synthetische Zeitreihe (Sinus + Rauschen)\")\n",
        "plt.xlabel(\"Zeit\")\n",
        "plt.ylabel(\"Wert\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yELH1fYQTVDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fensterbildung (Sequences) für RNN\n",
        "\n",
        "Ein RNN erwartet **Sequenzen** als Input, z.B.:\n",
        "\n",
        "- Eingabe:  $x\\_{t-29}, ..., x\\_{t}$   (Fensterlänge = 30)\n",
        "- Ziel:     $x\\_{t+1}$                (1-Schritt-Vorhersage)\n",
        "\n",
        "Wir definieren eine Funktion, die aus einem 1D-Tensor (Zeitreihe)\n",
        "Input-Sequenzen und Zielwerte erzeugt.\n",
        "\n",
        "Später können wir dieselbe Funktion auch für echte Zeitreihen verwenden.\n"
      ],
      "metadata": {
        "id": "h-hJvKALTej0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Hilfsfunktion: aus 1D-Serie Fenster (Input, Target) erzeugen\n",
        "\n",
        "def create_sequences(series: torch.Tensor, window_size: int, horizon: int = 1):\n",
        "    \"\"\"\n",
        "    Erzeugt Eingabe-/Ziel-Paare aus einer 1D-Zeitreihe.\n",
        "\n",
        "    series:      1D Tensor der Länge N\n",
        "    window_size: Anzahl der Vergangenheitswerte pro Input-Sequenz\n",
        "    horizon:     wie viele Schritte in die Zukunft vorhergesagt werden (hier: 1)\n",
        "\n",
        "    Rückgabe:\n",
        "        X: Tensor der Form (num_samples, window_size)\n",
        "        y: Tensor der Form (num_samples, horizon)\n",
        "    \"\"\"\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    N = len(series)\n",
        "    # Letzter Index, bei dem noch ein vollständiges Fenster + Horizon möglich ist\n",
        "    last_idx = N - window_size - horizon + 1\n",
        "\n",
        "    for start in range(last_idx):\n",
        "        end = start + window_size\n",
        "        target_start = end\n",
        "        target_end = end + horizon\n",
        "\n",
        "        x_window = series[start:end]\n",
        "        y_window = series[target_start:target_end]\n",
        "\n",
        "        xs.append(x_window)\n",
        "        ys.append(y_window)\n",
        "\n",
        "    X = torch.stack(xs)  # (num_samples, window_size)\n",
        "    y = torch.stack(ys)  # (num_samples, horizon)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "window_size = 30     # Länge des historischen Fensters\n",
        "horizon = 1          # 1-Schritt-Vorhersage\n",
        "\n",
        "X_all, y_all = create_sequences(series, window_size, horizon)\n",
        "X_all.shape, y_all.shape\n"
      ],
      "metadata": {
        "id": "Kfb2UObHTYeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train/Validation/Test-Split\n",
        "\n",
        "Wir teilen die generierten Fenster in drei Teile:\n",
        "\n",
        "- **Train**: für das Lernen der Gewichte  \n",
        "- **Validation**: zur Hyperparameter-Abstimmung  \n",
        "- **Test**: zur finalen Evaluation\n",
        "\n",
        "Wir verwenden einen einfachen Split nach Zeit (kein Mischen, um Leckage zu vermeiden).\n"
      ],
      "metadata": {
        "id": "ghDo28jaTz0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Zeitlicher Split der Sequenzen\n",
        "\n",
        "num_samples = X_all.shape[0]\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "train_end = int(num_samples * train_ratio)\n",
        "val_end = train_end + int(num_samples * val_ratio)\n",
        "\n",
        "X_train = X_all[:train_end]\n",
        "y_train = y_all[:train_end]\n",
        "\n",
        "X_val = X_all[train_end:val_end]\n",
        "y_val = y_all[train_end:val_end]\n",
        "\n",
        "X_test = X_all[val_end:]\n",
        "y_test = y_all[val_end:]\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "08VlV1rrTtu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Dataset- und DataLoader-Klassen\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Einfache Dataset-Klasse für Zeitreihenfenster.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        # X: (num_samples, window_size)\n",
        "        # y: (num_samples, horizon)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_seq = self.X[idx]  # (window_size,)\n",
        "        y_val = self.y[idx]  # (horizon,)\n",
        "\n",
        "        # RNN/LSTM erwartet Input der Form (batch, seq_len, input_size)\n",
        "        # Hier: input_size = 1, also wir erweitern die letzte Dimension\n",
        "        x_seq = x_seq.unsqueeze(-1)  # (window_size, 1)\n",
        "\n",
        "        return x_seq, y_val\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ef4zla4sT3P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. LSTM-Modell definieren\n",
        "\n",
        "Wir verwenden ein einfaches LSTM (statt plain RNN), weil es in der Praxis\n",
        "bei längeren Sequenzen stabiler ist.\n",
        "\n",
        "**Eingabeform**:  \n",
        "- Batch:   \\(B\\)  \n",
        "- Zeit:    \\(T\\) (Fensterlänge)  \n",
        "- Feature: \\(F = 1\\) (ein Skalar pro Zeitschritt)\n",
        "\n",
        "LSTM-Ausgabe:  \n",
        "- Wir nehmen den **letzten Zeitschritt** der Hidden States und leiten ihn\n",
        "  über ein Fully-Connected-Layer auf die gewünschte Horizon-Dimension (hier: 1) ab.\n"
      ],
      "metadata": {
        "id": "_-bkBDfqUNmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fOYiDk3v4nem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 LSTM-Modellklasse\n",
        "\n",
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_size: int = 1, hidden_size: int = 32, num_layers: int = 1, output_size: int = 1):\n",
        "        \"\"\"\n",
        "        Einfacher LSTM-basierter Forecaster.\n",
        "\n",
        "        input_size:  Dimension der Eingabefeatures (hier 1)\n",
        "        hidden_size: Größe des verborgenen Zustands\n",
        "        num_layers:  Anzahl der LSTM-Schichten\n",
        "        output_size: Anzahl der vorherzusagenden Werte (Horizon)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True  # (batch, seq_len, input_size)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor der Form (batch, seq_len, input_size)\n",
        "        \"\"\"\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        # lstm_out: (batch, seq_len, hidden_size)\n",
        "\n",
        "        # Wir nehmen den letzten Zeitschritt der Ausgabe\n",
        "        last_output = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
        "\n",
        "        # Lineare Projektion auf die Zielgröße\n",
        "        out = self.fc(last_output)       # (batch, output_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = LSTMForecaster(input_size=1, hidden_size=32, num_layers=1, output_size=horizon).to(device)\n",
        "model\n"
      ],
      "metadata": {
        "id": "qybVD0nWT8X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5a. Verständis von LSTM\n",
        "\n",
        "Dieses Kapitel erklärt, was ein LSTM im Vorwärtslauf zurückgibt und wie diese Werte zu interpretieren sind. Die Darstellung ist bewusst klar und didaktisch gehalten.\n",
        "\n",
        "### 1. Was gibt ein LSTM zurück?\n",
        "Verständnis der Zeile `lstm_out, (h_n, c_n) = self.lstm(x)`\n",
        "\n",
        "**Input** $x$ für LSTM:\n",
        "\n",
        "1. batch size = B\n",
        "2. sequence length = T\n",
        "3. feature size = F\n",
        "\n",
        "$x$ hat die Form (B, T, F)\n",
        "\n",
        "**Output**: Ein LSTM in PyTorch liefert zwei Hauptbestandteile:\n",
        "\n",
        "\n",
        "1. `lstm_out` – die Hidden States für jeden Zeitschritt  \n",
        "2. `(h_n, c_n)` – das finale Hidden- und Cell-State nach dem letzten Zeitschritt\n",
        "\n",
        "Typische Zeile:\n",
        "\n",
        "```python\n",
        "lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "```\n",
        "\n",
        "### 2. Bedeutung von `lstm_out`\n",
        "\n",
        "Ein LSTM erzeugt für jeden Zeitschritt ein Hidden State.  \n",
        "`lstm_out` enthält alle diese Hidden States.\n",
        "\n",
        "Form:\n",
        "\n",
        "```\n",
        "lstm_out.shape = (Batch, Seq_Length, Hidden_Size)\n",
        "```\n",
        "\n",
        "Für Forecasting wird meist nur der letzte Zeitschritt verwendet:\n",
        "\n",
        "```python\n",
        "last_output = lstm_out[:, -1, :]\n",
        "```\n",
        "\n",
        "### 3. Bedeutung von `h_n` (Hidden State)\n",
        "\n",
        "`h_n` ist der finale Hidden State:\n",
        "\n",
        "```\n",
        "h_n.shape = (num_layers, Batch, Hidden_Size)\n",
        "```\n",
        "\n",
        "Eigenschaften:\n",
        "- Kurzzeitgedächtnis  \n",
        "- Direkter Repräsentant der Sequenz  \n",
        "- Wird zur Ausgabe weiterverwendet\n",
        "\n",
        "### 4. Bedeutung von `c_n` (Cell State)\n",
        "\n",
        "`c_n` ist der finale Zustand des Langzeitgedächtnisses.\n",
        "\n",
        "```\n",
        "c_n.shape = (num_layers, Batch, Hidden_Size)\n",
        "```\n",
        "\n",
        "Eigenschaften:\n",
        "- Langzeitgedächtnis  \n",
        "- Transportiert Information über längere Zeit  \n",
        "- Stabilisiert das Lernen gegenüber RNN\n",
        "\n",
        "### 5. Warum gibt es zwei Zustände?\n",
        "\n",
        "LSTM besitzt zwei Speicher:\n",
        "\n",
        "1. Hidden State `h_t` – kurzfristige Information  \n",
        "2. Cell State `c_t` – langfristige Information\n",
        "\n",
        "Diese Zweiteilung unterscheidet LSTM wesentlich von einfachen RNNs.\n",
        "\n",
        "### 6. Formen der Rückgabewerte\n",
        "\n",
        "Wenn:\n",
        "- `num_layers = L`  \n",
        "- `hidden_size = H`  \n",
        "- Batch Size = B  \n",
        "\n",
        "dann:\n",
        "\n",
        "```\n",
        "lstm_out: (B, T, H)\n",
        "h_n:      (L, B, H)\n",
        "c_n:      (L, B, H)\n",
        "```\n",
        "\n",
        "### 7. Mehrschichtiges LSTM (Stacked LSTM)\n",
        "\n",
        "Beispiel:\n",
        "\n",
        "```python\n",
        "nn.LSTM(input_size=1, hidden_size=32, num_layers=2)\n",
        "```\n",
        "\n",
        "Dies bedeutet:\n",
        "- Zwei LSTM-Schichten übereinander  \n",
        "- Jede Schicht hat eigene Hidden- und Cell-States\n",
        "\n",
        "Visualisierung:\n",
        "\n",
        "```\n",
        "Zeit t:   Layer1 -> Layer2\n",
        "Zeit t+1: Layer1 -> Layer2\n",
        "```\n",
        "\n",
        "### 8. Unterschied RNN vs. LSTM\n",
        "\n",
        "Einfaches RNN:\n",
        "- Nur ein Zustand `h_t`  \n",
        "- Vergisst schnell\n",
        "\n",
        "LSTM:\n",
        "- Zwei Zustände `h_t` und `c_t`  \n",
        "- Kann Langzeitabhängigkeiten abbilden  \n",
        "- Besser für Zeitreihen geeignet\n",
        "\n",
        "### 9. Zusammenfassung\n",
        "\n",
        "- `lstm_out` enthält Hidden States aller Zeitschritte  \n",
        "- `h_n` ist der letzte Hidden State  \n",
        "- `c_n` ist der letzte Cell State  \n",
        "- LSTM hat Kurz- und Langzeitgedächtnis  \n",
        "- Mehrschichtiges LSTM stapelt mehrere LSTM-Blöcke  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T91iQiho40Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training vorbereiten\n",
        "\n",
        "- Loss-Funktion: Mean Squared Error (MSE)  \n",
        "- Optimizer: Adam  \n",
        "- Epochen: z.B. 30\n",
        "\n",
        "Wir werten pro Epoche auch die Validation-Loss aus.\n"
      ],
      "metadata": {
        "id": "RSKrTF1SU6iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Loss und Optimizer\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 30\n"
      ],
      "metadata": {
        "id": "TTQOqkAlU5sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2 Trainingsschleife\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.to(device)            # (batch, seq_len, 1)\n",
        "            y_batch = y_batch.to(device)            # (batch, horizon)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred = model(x_batch)                 # (batch, horizon)\n",
        "\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                y_pred = model(x_batch)\n",
        "                loss = criterion(y_pred, y_batch)\n",
        "                running_val_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        print(f\"Epoche {epoch:02d} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n"
      ],
      "metadata": {
        "id": "ubT_aFsWUyYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3 Verlauf der Trainings- und Validierungs-Fehler\n",
        "\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel(\"MSE-Loss\")\n",
        "plt.title(\"Trainingsverlauf\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n5lDhdzlVAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation auf dem Test-Set\n",
        "\n",
        "Wir lassen das trainierte Modell Vorhersagen auf dem Test-Set machen und\n",
        "vergleichen sie mit den echten Werten der Zeitreihe.\n"
      ],
      "metadata": {
        "id": "y2dOyundVMZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.1 Vorhersage-Funktion\n",
        "\n",
        "def predict_on_loader(model, data_loader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            preds.append(y_pred.cpu())\n",
        "            targets.append(y_batch.cpu())\n",
        "\n",
        "    preds = torch.cat(preds, dim=0)     # (num_samples, horizon)\n",
        "    targets = torch.cat(targets, dim=0) # (num_samples, horizon)\n",
        "    return preds, targets\n",
        "\n",
        "\n",
        "y_test_pred, y_test_true = predict_on_loader(model, test_loader)\n",
        "y_test_pred.shape, y_test_true.shape\n"
      ],
      "metadata": {
        "id": "9uqeBs3zVI2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2 Visualisierung: Test-Vorhersagen vs. wahre Werte\n",
        "\n",
        "# Wir nehmen hier die 1-Schritt-Vorhersage und plotten sie\n",
        "pred_np = y_test_pred.squeeze(-1).numpy()\n",
        "true_np = y_test_true.squeeze(-1).numpy()\n",
        "\n",
        "plt.plot(true_np, label=\"True\")\n",
        "plt.plot(pred_np, label=\"Prediction\")\n",
        "plt.title(\"Test-Set: 1-Schritt-Vorhersage\")\n",
        "plt.xlabel(\"Zeitschritt (Test-Fenster-Index)\")\n",
        "plt.ylabel(\"Wert\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7MCuCguUVPXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Aufgaben für Studierende\n",
        "\n",
        "1. **Fensterlänge verändern**  \n",
        "   - Ändern Sie `window_size` (z.B. 10, 50, 100).  \n",
        "   - Erneut trainieren und beobachten, wie sich Training und Test-Fehler verändern.\n",
        "\n",
        "2. **Hidden Size und Layers variieren**  \n",
        "   - Probieren Sie `hidden_size = 16` und `hidden_size = 64`.  \n",
        "   - Fügen Sie eine zweite LSTM-Schicht hinzu (`num_layers = 2`).  \n",
        "   - Diskutieren Sie Overfitting / Underfitting anhand der Verlaufsplots.\n",
        "\n",
        "3. **RNN statt LSTM** (optional)  \n",
        "   - Ersetzen Sie `nn.LSTM` durch `nn.RNN` und passen Sie den Code an.  \n",
        "   - Vergleichen Sie die Ergebnisse (Stabilität, Loss, Verlauf).\n",
        "\n",
        "4. **Mehrschritt-Vorhersage** (optional)  \n",
        "   - Setzen Sie `horizon = 5` und passen Sie Modell und Visualisierung an.  \n",
        "   - Lassen Sie das Modell 5 Schritte in die Zukunft vorhersagen.\n",
        "\n",
        "5. **Eigene Zeitreihe**  \n",
        "   - Nutzen Sie den Abschnitt mit `yfinance` unten, um z.B. den S&P500 zu laden.  \n",
        "   - Verwenden Sie die gleiche Fensterlogik (`create_sequences`) und dasselbe Modell.  \n",
        "   - Hinweis: Finanzzeitreihen sind extrem verrauscht. Erwarten Sie keine gute Vorhersage –  \n",
        "     Ziel ist das methodische Verständnis, nicht ein Trading-System.\n"
      ],
      "metadata": {
        "id": "AaOnfBk8VYph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Eigene Zeitreihe (z. B. S&P 500 mit yfinance)\n",
        "\n",
        "In diesem Abschnitt zeigen wir, wie man eine **reale Zeitreihe** (z.B. den S&P500 Index)\n",
        "herunterlädt und mit derselben RNN-Pipeline verarbeitet.\n",
        "\n",
        "> Hinweis:  \n",
        "> - Für Finanzzeitreihen ist 1-Schritt-Forecasting mit so einfachen Modellen meist **schwach**.  \n",
        "> - Nehmen Sie das als experimentelles Beispiel, nicht als Investitions-Ratgeber :-)\n"
      ],
      "metadata": {
        "id": "go5dybQBVnYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.1 Installation von yfinance (in Colab meist notwendig)\n",
        "# In Jupyter/Colab einmal ausführen, in lokalen Umgebungen ggf. in der Shell installieren.\n",
        "#!pip install yfinance\n",
        "\n",
        "# 9.2 Daten mit yfinance laden\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "# Beispiel: S&P 500 Index (^GSPC)\n",
        "sp500 = yf.download(\"^GSPC\", start=\"2015-01-01\", end=None)\n",
        "\n",
        "sp500.head()\n"
      ],
      "metadata": {
        "id": "CjqtHwbGVSLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.3 Einfaches Preprocessing: wir nehmen den Close\n",
        "# und normalisieren ihn grob.\n",
        "\n",
        "close = sp500[\"Close\"].dropna()\n",
        "close_values = close.values.astype(np.float32)\n",
        "\n",
        "# In Torch-Tensor umwandeln\n",
        "sp_series = torch.tensor(close_values, dtype=torch.float32).squeeze(-1)\n",
        "sp_series.shape\n",
        "\n",
        "# Optional: Normierung (z.B. z-Score)\n",
        "mean_sp = sp_series.mean()\n",
        "std_sp = sp_series.std()\n",
        "\n",
        "sp_series_norm = (sp_series - mean_sp) / std_sp\n",
        "sp_series_norm.shape\n"
      ],
      "metadata": {
        "id": "haiYVNPqVxms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.4 Fenster und Split erzeugen (wie oben)\n",
        "\n",
        "window_size_sp = 30\n",
        "horizon_sp = 1\n",
        "\n",
        "X_sp, y_sp = create_sequences(sp_series_norm, window_size_sp, horizon_sp)\n",
        "\n",
        "### Lösung hier"
      ],
      "metadata": {
        "id": "xCcNSxQ-VsNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.5 Neues Modell für S&P500 trainieren\n",
        "\n",
        "### Lösung hier\n"
      ],
      "metadata": {
        "id": "uy2ZJI_DWHMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VR4-y0gWRVw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}