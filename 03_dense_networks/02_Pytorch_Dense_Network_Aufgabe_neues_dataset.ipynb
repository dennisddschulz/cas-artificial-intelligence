{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "training_data = datasets.FGVCAircraft(\n",
    "    root=\"data\",\n",
    "    split=\"train\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.FGVCAircraft(\n",
    "    root=\"data\",\n",
    "    split=\"test\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IufCioy479R",
    "outputId": "919c30d6-a2cd-4170-b9e9-4e748518577b",
    "ExecuteTime": {
     "end_time": "2025-11-05T10:45:29.857029Z",
     "start_time": "2025-11-05T10:45:29.854800Z"
    }
   },
   "source": [
    "print(training_data)\n",
    "print(test_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FGVCAircraft\n",
      "    Number of datapoints: 3334\n",
      "    Root location: data\n",
      "    StandardTransform\n",
      "Transform: ImageClassification(\n",
      "               crop_size=[224]\n",
      "               resize_size=[256]\n",
      "               mean=[0.485, 0.456, 0.406]\n",
      "               std=[0.229, 0.224, 0.225]\n",
      "               interpolation=InterpolationMode.BILINEAR\n",
      "           )\n",
      "Dataset FGVCAircraft\n",
      "    Number of datapoints: 3333\n",
      "    Root location: data\n",
      "    StandardTransform\n",
      "Transform: ImageClassification(\n",
      "               crop_size=[224]\n",
      "               resize_size=[256]\n",
      "               mean=[0.485, 0.456, 0.406]\n",
      "               std=[0.229, 0.224, 0.225]\n",
      "               interpolation=InterpolationMode.BILINEAR\n",
      "           )\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "xfnPCzYdapkS",
    "outputId": "b7566015-df69-44b3-87c3-dc4fe61f33fa",
    "ExecuteTime": {
     "end_time": "2025-11-05T09:40:56.683076Z",
     "start_time": "2025-11-05T09:40:56.679629Z"
    }
   },
   "source": [
    "'''\n",
    "1. Torch tensor\n",
    "train_target = torch.tensor(train['Target'].values.astype(np.float32)) # train ist ein DF\n",
    "train = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32))\n",
    "\n",
    "2. Torch dataset\n",
    "train_tensor = data_utils.TensorDataset(train, train_target)\n",
    "test_tensor = data_utils.TensorDataset(test, test_target)\n",
    "\n",
    "3. Torch dataloader\n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)\n",
    "'''\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. Torch tensor\\ntrain_target = torch.tensor(train['Target'].values.astype(np.float32)) # train ist ein DF\\ntrain = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32))\\n\\n2. Torch dataset\\ntrain_tensor = data_utils.TensorDataset(train, train_target)\\ntest_tensor = data_utils.TensorDataset(test, test_target)\\n\\n3. Torch dataloader\\ntrain_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\\ntest_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Dataloaders will be created after hyperparameters are set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:45:37.260630Z",
     "start_time": "2025-11-05T10:45:34.361454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model suitable for FGVCAircraft (100 classes, 224x224 RGB)\n",
    "num_classes = len(training_data.classes)\n",
    "# Use a pretrained ResNet18 and replace the final fully connected layer\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "model = resnet.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/isc-den/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:45:42.094067Z",
     "start_time": "2025-11-05T10:45:42.092442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:45:44.033551Z",
     "start_time": "2025-11-05T10:45:44.031677Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:45:45.920799Z",
     "start_time": "2025-11-05T10:45:45.918394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:45:48.873437Z",
     "start_time": "2025-11-05T10:45:48.870914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy*100:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T12:31:01.235856Z",
     "start_time": "2025-11-05T12:26:49.953854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 2\n",
    "# Create dataloaders\n",
    "loader_kwargs = {\"batch_size\": batch_size, \"shuffle\": True}\n",
    "if device == \"cuda\":\n",
    "    loader_kwargs.update({\"num_workers\": 2, \"pin_memory\": True})\n",
    "train_dataloader = DataLoader(training_data, **loader_kwargs)\n",
    "loader_kwargs[\"shuffle\"] = False\n",
    "test_dataloader = DataLoader(test_data, **loader_kwargs)\n",
    "\n",
    "# Quick sanity check\n",
    "X0, y0 = next(iter(train_dataloader))\n",
    "print(\"Batch shape:\", X0.shape, \"Labels shape:\", y0.shape)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 3, 224, 224]) Labels shape: torch.Size([32])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.796173  [   32/ 3334]\n",
      "loss: 1.465021  [ 1632/ 3334]\n",
      "loss: 1.366227  [ 3232/ 3334]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.146276 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.798728  [   32/ 3334]\n",
      "loss: 0.518741  [ 1632/ 3334]\n",
      "loss: 0.469396  [ 3232/ 3334]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.981782 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T12:26:46.473065Z",
     "start_time": "2025-11-05T12:26:46.334928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload model for inference\n",
    "inference_model = models.resnet18(weights=None)\n",
    "inference_model.fc = nn.Linear(inference_model.fc.in_features, num_classes)\n",
    "inference_model = inference_model.to(device)\n",
    "inference_model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "inference_model.eval()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"linear_relu_stack.0.weight\", \"linear_relu_stack.0.bias\", \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m inference_model.fc = nn.Linear(inference_model.fc.in_features, num_classes)\n\u001B[32m      4\u001B[39m inference_model = inference_model.to(device)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43minference_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m inference_model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/cas-artificial-intelligence/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2629\u001B[39m, in \u001B[36mModule.load_state_dict\u001B[39m\u001B[34m(self, state_dict, strict, assign)\u001B[39m\n\u001B[32m   2621\u001B[39m         error_msgs.insert(\n\u001B[32m   2622\u001B[39m             \u001B[32m0\u001B[39m,\n\u001B[32m   2623\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2624\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[32m   2625\u001B[39m             ),\n\u001B[32m   2626\u001B[39m         )\n\u001B[32m   2628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2629\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   2630\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2631\u001B[39m             \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m\"\u001B[39m.join(error_msgs)\n\u001B[32m   2632\u001B[39m         )\n\u001B[32m   2633\u001B[39m     )\n\u001B[32m   2634\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[31mRuntimeError\u001B[39m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"linear_relu_stack.0.weight\", \"linear_relu_stack.0.bias\", \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\". "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T12:26:42.042043Z",
     "start_time": "2025-11-05T12:26:41.962842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use dataset's class names\n",
    "classes = training_data.classes\n",
    "\n",
    "# Single-sample inference\n",
    "x, y = test_data[0]\n",
    "with torch.no_grad():\n",
    "    x = x.unsqueeze(0).to(device)  # add batch dim\n",
    "    logits = inference_model(x)\n",
    "    pred_idx = int(logits.argmax(1).item())\n",
    "    predicted, actual = classes[pred_idx], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inference_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[39]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m      7\u001B[39m     x = x.unsqueeze(\u001B[32m0\u001B[39m).to(device)  \u001B[38;5;66;03m# add batch dim\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     logits = \u001B[43minference_model\u001B[49m(x)\n\u001B[32m      9\u001B[39m     pred_idx = \u001B[38;5;28mint\u001B[39m(logits.argmax(\u001B[32m1\u001B[39m).item())\n\u001B[32m     10\u001B[39m     predicted, actual = classes[pred_idx], classes[y]\n",
      "\u001B[31mNameError\u001B[39m: name 'inference_model' is not defined"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Number of test samples:\", len(test_data))\n",
    "print(\"Sample tensor shape:\", test_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3VqAoVbUeTtt"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
