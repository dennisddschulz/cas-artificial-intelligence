{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "003e5d42",
      "metadata": {
        "id": "003e5d42"
      },
      "source": [
        "# Dense Networks Â· SP500 Nextâ€‘Day Direction (Binary Classification)\n",
        "\n",
        "**Objective**: Predict tomorrow's SP500 move (up/down) using **lagged crossâ€‘sectional stock returns** with a **Dense (MLP) network**.\n",
        "\n",
        "You can run this notebook in **Google Colab**. It includes two ready experiments:\n",
        "- **A:** Few stocks, many lags  \n",
        "- **B:** Many stocks, few lags\n",
        "\n",
        "ðŸ‡¬ðŸ‡§ Note: This exercise belongs to the Dense Networks topic. Students compare how the number of lags and the number of stocks affect the modelâ€™s stability and performance.\n",
        "\n",
        "ðŸ‡·ðŸ‡º ÐŸÐ¾Ð´ÑÐºÐ°Ð·ÐºÐ°: ÑÑ‚Ð¾ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒÐ¼ Ð´Ð»Ñ Ñ‚ÐµÐ¼Ñ‹ *Dense Networks*. Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚Ñ‹ ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÑŽÑ‚ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ **Ñ‡Ð¸ÑÐ»Ð° Ð»Ð°Ð³Ð¾Ð²** Ð¸ **Ñ‡Ð¸ÑÐ»Ð° Ð°ÐºÑ†Ð¸Ð¹** Ð½Ð° ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚ÑŒ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac3ea89",
      "metadata": {
        "id": "2ac3ea89"
      },
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e38ed6",
      "metadata": {
        "id": "e7e38ed6"
      },
      "outputs": [],
      "source": [
        "# If running in Colab:\n",
        "# !pip -q install yfinance pandas numpy scikit-learn torch==2.4.1 matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314ffb0d",
      "metadata": {
        "id": "314ffb0d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.manual_seed(7)\n",
        "np.random.seed(7)\n",
        "\n",
        "print(\"Torch:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd1f1308",
      "metadata": {
        "id": "fd1f1308"
      },
      "source": [
        "## 1) Data: choose one of the options\n",
        "\n",
        "### Option 1 â€” Download with `yfinance` (easiest in Colab)\n",
        "- Includes **SPY** for market proxy and a basket of largeâ€‘cap tickers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2792d992",
      "metadata": {
        "id": "2792d992"
      },
      "outputs": [],
      "source": [
        "use_yfinance = True  # set to False if you upload your own CSV of returns\n",
        "\n",
        "tickers_all = [\n",
        "    \"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOG\",\"META\",\"BRK-B\",\"TSLA\",\"AVGO\",\"LLY\",\n",
        "    \"V\",\"JPM\",\"XOM\",\"UNH\",\"MA\",\"HD\",\"PG\",\"COST\",\"ADBE\",\"NFLX\",\n",
        "    \"PEP\",\"CRM\",\"KO\",\"MRK\",\"ABBV\"\n",
        "]\n",
        "spy = \"SPY\"\n",
        "start_date = \"2012-01-01\"\n",
        "end_date = None  # or e.g. \"2025-01-01\"\n",
        "\n",
        "if use_yfinance:\n",
        "    try:\n",
        "        import yfinance as yf\n",
        "        data = yf.download([spy] + tickers_all, start=start_date, end=end_date, auto_adjust=True, progress=False)[\"Close\"]\n",
        "        data = data.dropna(how=\"all\")\n",
        "        data = data.asfreq(\"B\").ffill()  # align to business days\n",
        "        rets = data.pct_change().dropna()\n",
        "        rets.columns = [c.replace(\" \", \"_\").replace(\"-\", \"_\") for c in rets.columns]\n",
        "        spy_col = spy\n",
        "        print(\"Data downloaded. Shape:\", rets.shape)\n",
        "    except Exception as e:\n",
        "        print(\"yfinance failed:\", e)\n",
        "        use_yfinance = False\n",
        "else:\n",
        "    print(\"Set use_yfinance=True to auto-download, or upload your own CSV in the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75567bc5",
      "metadata": {
        "id": "75567bc5"
      },
      "source": [
        "### Option 2 â€” Upload your own **returns** CSV\n",
        "- Must contain **SPY** column (market proxy) and several stock columns\n",
        "- Index must be a **date** (parseable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7f1c72",
      "metadata": {
        "id": "5f7f1c72"
      },
      "outputs": [],
      "source": [
        "# If not using yfinance, upload your own daily RETURNS CSV with columns: SPY, AAPL, MSFT, ...\n",
        "# Example:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# rets = pd.read_csv(list(uploaded.keys())[0], index_col=0, parse_dates=True)\n",
        "# spy_col = \"SPY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88cef556",
      "metadata": {
        "id": "88cef556"
      },
      "outputs": [],
      "source": [
        "assert 'rets' in globals(), \"Please prepare `rets` (returns DataFrame) and `spy_col`.\"\n",
        "assert spy_col in rets.columns, f\"`{spy_col}` column required.\"\n",
        "rets = rets.sort_index()\n",
        "rets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c626189",
      "metadata": {
        "id": "0c626189"
      },
      "source": [
        "## 2) Target and Base Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9fb250",
      "metadata": {
        "id": "bf9fb250"
      },
      "outputs": [],
      "source": [
        "# Target = tomorrow's SP500/market sign\n",
        "y = (rets[spy_col].shift(-1) > 0).astype(int)\n",
        "\n",
        "# Base cross-sectional features (exclude SPY to avoid leakage)\n",
        "X_base = rets.drop(columns=[spy_col])\n",
        "X_base.head(), y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d8575c9",
      "metadata": {
        "id": "8d8575c9"
      },
      "source": [
        "## 3) Helper: build lagged features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2244a73",
      "metadata": {
        "id": "b2244a73"
      },
      "outputs": [],
      "source": [
        "def build_lagged_features(df: pd.DataFrame, n_lags: int) -> pd.DataFrame:\n",
        "    cols = {}\n",
        "    for lag in range(1, n_lags + 1):\n",
        "        lagged = df.shift(lag).add_suffix(f\"_lag{lag}\")\n",
        "        cols[lag] = lagged\n",
        "    X = pd.concat([cols[lag] for lag in cols], axis=1)\n",
        "    return X\n",
        "\n",
        "# Quick test\n",
        "_ = build_lagged_features(X_base.iloc[:10], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb3b28a2",
      "metadata": {
        "id": "bb3b28a2"
      },
      "source": [
        "## 4) PyTorch dataset & model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94547b2",
      "metadata": {
        "id": "c94547b2"
      },
      "outputs": [],
      "source": [
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32).reshape(-1, 1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden=(64, 32), p_dropout=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(p_dropout)]\n",
        "            last = h\n",
        "        layers += [nn.Linear(last, 1)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a929bc35",
      "metadata": {
        "id": "a929bc35"
      },
      "outputs": [],
      "source": [
        "def train_model(X_train, y_train, X_val, y_val, epochs=50, batch_size=128, lr=1e-3, weight_decay=1e-4, hidden=(64,32)):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_val_s   = scaler.transform(X_val)\n",
        "\n",
        "    train_ds = TabDataset(X_train_s, y_train)\n",
        "    val_ds   = TabDataset(X_val_s,   y_val)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = MLP(in_dim=X_train.shape[1], hidden=hidden, p_dropout=0.1)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    patience, patience_left = 8, 8\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * len(xb)\n",
        "        train_loss /= len(train_ds)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            preds = []\n",
        "            ys = []\n",
        "            for xb, yb in val_loader:\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_loss += loss.item() * len(xb)\n",
        "                preds.append(torch.sigmoid(logits).cpu().numpy())\n",
        "                ys.append(yb.cpu().numpy())\n",
        "            val_loss /= len(val_ds)\n",
        "        if val_loss < best_val - 1e-5:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience_left = patience\n",
        "        else:\n",
        "            patience_left -= 1\n",
        "            if patience_left <= 0:\n",
        "                # early stop\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203e3736",
      "metadata": {
        "id": "203e3736"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, scaler, X, y, threshold=0.5, label=\"VAL\"):\n",
        "    Xs = scaler.transform(X).astype(np.float32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(Xs))\n",
        "        probs = torch.sigmoid(logits).numpy().ravel()\n",
        "\n",
        "    y_pred = (probs >= threshold).astype(int)\n",
        "    acc  = accuracy_score(y, y_pred)\n",
        "    bacc = balanced_accuracy_score(y, y_pred)\n",
        "    try:\n",
        "        roc  = roc_auc_score(y, probs)\n",
        "    except ValueError:\n",
        "        roc = np.nan\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "    print(f\"[{label}] Accuracy={acc:.3f} | BalancedAcc={bacc:.3f} | ROC-AUC={roc:.3f}\")\n",
        "    print(\"Confusion matrix:\\n\", cm)\n",
        "    return {\"acc\": acc, \"bacc\": bacc, \"roc\": roc, \"cm\": cm, \"probs\": probs, \"pred\": y_pred}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3fd560a",
      "metadata": {
        "id": "e3fd560a"
      },
      "source": [
        "## 5) Time split: train / val / test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1d66c4",
      "metadata": {
        "id": "dd1d66c4"
      },
      "outputs": [],
      "source": [
        "def time_split_idx(n, train_frac=0.7, val_frac=0.15):\n",
        "    train_end = int(n * train_frac)\n",
        "    val_end   = int(n * (train_frac + val_frac))\n",
        "    return slice(0, train_end), slice(train_end, val_end), slice(val_end, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25d07c8",
      "metadata": {
        "id": "c25d07c8"
      },
      "source": [
        "## 6) Experiments A & B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a8e76b",
      "metadata": {
        "id": "b8a8e76b"
      },
      "outputs": [],
      "source": [
        "def run_experiment(name, stock_list, n_lags, hidden=(64,32), epochs=60):\n",
        "    print(f\"\\n=== Experiment {name}: stocks={len(stock_list)}, lags={n_lags} ===\")\n",
        "    X_cs = X_base[stock_list]\n",
        "    X = build_lagged_features(X_cs, n_lags)\n",
        "    y_aligned = y.loc[X.index]\n",
        "\n",
        "    # drop NA from shifts\n",
        "    mask = X.notna().all(axis=1) & y_aligned.notna()\n",
        "    X = X[mask]\n",
        "    y_arr = y_aligned[mask].values\n",
        "\n",
        "    n = len(X)\n",
        "    tr, va, te = time_split_idx(n, 0.7, 0.15)\n",
        "    X_train, y_train = X.iloc[tr].values, y_arr[tr]\n",
        "    X_val,   y_val   = X.iloc[va].values, y_arr[va]\n",
        "    X_test,  y_test  = X.iloc[te].values, y_arr[te]\n",
        "\n",
        "    model, scaler = train_model(X_train, y_train, X_val, y_val, epochs=epochs, hidden=hidden)\n",
        "    eval_val = evaluate_model(model, scaler, X_val, y_val, label=f\"{name}-VAL\")\n",
        "    eval_te  = evaluate_model(model, scaler, X_test, y_test, label=f\"{name}-TEST\")\n",
        "\n",
        "    return {\"name\": name, \"eval_val\": eval_val, \"eval_test\": eval_te}\n",
        "\n",
        "# Define stock subsets\n",
        "few_stocks  = [\"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOG\"]\n",
        "many_stocks = tickers_all if 'tickers_all' in globals() else list(X_base.columns)[:20]\n",
        "\n",
        "# Run two contrasting experiments\n",
        "res_A = run_experiment(\"A (few stocks, many lags)\", stock_list=few_stocks,  n_lags=5, hidden=(64,32))\n",
        "res_B = run_experiment(\"B (many stocks, few lags)\", stock_list=many_stocks, n_lags=2, hidden=(128,64))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§© Concept Notes\n",
        "\n",
        "#### **Biasâ€“Variance Trade-off**\n",
        "When we increase model complexity â€” for example, by adding more **lags**, **neurons**, or **layers** â€” the model can fit the training data more precisely but may lose its ability to generalize.  \n",
        "This balance between **bias** (systematic error) and **variance** (sensitivity to noise) is central to all machine learning:\n",
        "\n",
        "| Scenario | Bias | Variance | Description |\n",
        "|-----------|------|-----------|--------------|\n",
        "| Too simple model (few lags, small net) | high | low | underfitting â€” misses true patterns |\n",
        "| Too complex model (many lags, deep net) | low | high | overfitting â€” memorizes noise |\n",
        "| Balanced model | moderate | moderate | captures signal, ignores noise |\n",
        "\n",
        "> ðŸ’¡ In this exercise: adding too many lags can make the model memorize daily fluctuations instead of learning meaningful temporal structure.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Cross-sectional Breadth**\n",
        "Cross-sectional means â€œacross many assets at the same time.â€  \n",
        "If **lags** add *temporal depth*, then **breadth** adds *market width* â€” the diversity of stocks used as simultaneous features.\n",
        "\n",
        "| Scenario | What it means | Effect |\n",
        "|-----------|----------------|---------|\n",
        "| Few stocks, many lags | Focus on temporal behavior of each stock | Captures time-series structure but loses market context |\n",
        "| Many stocks, few lags | Focus on market snapshot at each date | Captures cross-sectional structure but adds noise |\n",
        "\n",
        "> ðŸ’¡ Example: if the market index rises because only tech stocks surge while others fall, this is a **cross-sectional signal** â€” it canâ€™t be seen from SP500 alone.\n"
      ],
      "metadata": {
        "id": "GOlyWlm2tmLl"
      },
      "id": "GOlyWlm2tmLl"
    },
    {
      "cell_type": "markdown",
      "id": "9f14fbfb",
      "metadata": {
        "id": "9f14fbfb"
      },
      "source": [
        "## 7) Discuss\n",
        "\n",
        "EN:\n",
        "- **Biasâ€“Variance Trade-off**: Increasing the number of lags (time dimension) while keeping the same number of stocks expands the feature space â€” this raises the risk of overfitting, especially with shorter time series.\n",
        "\n",
        "- **Cross-sectional Breadth**: Using more stocks with fewer lags adds cross-sectional information (market dispersion on a given day) but may introduce additional noise.\n",
        "\n",
        "- **Metrics â‰ˆ 0.50â€“0.55** on test data are typical for a clean setup without data leakage.\n",
        "If ROC-AUC < 0.5, the model has learned a weak **anti-signal** â€” try inverting the target or adjusting feature definitions.\n",
        "\n",
        "\n",
        "RU:\n",
        "- **Biasâ€“Variance tradeâ€‘off**: Ð±Ð¾Ð»ÑŒÑˆÐµ Ð»Ð°Ð³Ð¾Ð² (Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸) Ð¿Ñ€Ð¸ Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ Ð°ÐºÑ†Ð¸Ð¹ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð° â†’ Ñ€Ð¸ÑÐº Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… Ñ€ÑÐ´Ð°Ñ….  \n",
        "- **Crossâ€‘sectional breadth**: Ð±Ð¾Ð»ÑŒÑˆÐµ Ð°ÐºÑ†Ð¸Ð¹ Ð¿Ñ€Ð¸ Ð¼Ð°Ð»Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ Ð»Ð°Ð³Ð¾Ð² Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ð¿Ð¾Ð¿ÐµÑ€ÐµÑ‡Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ (Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ€Ñ‹Ð½ÐºÐ° Ð² ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ), Ð½Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ ÑƒÑÐ¸Ð»Ð¸Ð²Ð°Ñ‚ÑŒ ÑˆÑƒÐ¼.  \n",
        "- **ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ â‰ˆ 0.50â€“0.55** Ð½Ð° Ñ‚ÐµÑÑ‚Ðµ â€” ÑÑ‚Ð¾ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Â«Ñ‡ÐµÑÑ‚Ð½Ð¾Ð¹Â» Ð¿Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ð±ÐµÐ· ÑƒÑ‚ÐµÑ‡ÐºÐ¸. Ð•ÑÐ»Ð¸ ROCâ€‘AUC < 0.5 â€” ÑÑ‚Ð¾ **Ð°Ð½Ñ‚Ð¸â€‘ÑÐ¸Ð³Ð½Ð°Ð»**; Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¸Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ€Ð¾Ð³ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}